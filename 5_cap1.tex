% !TeX root = Tesis.tex
% !TeX spellcheck = es_ES
% !TeX encoding = UTF-8
\chapter{Materiales y métodos}
\label{cap1}
\section{Reducción de la dimensionalidad}
\label{c11}
\onehalfspacing

%La reducción de la dimensionalidad es un paso crucial en el análisis de datos, especialmente cuando se trata de grandes conjuntos de datos con muchas variables. Al reducir la dimensionalidad, se facilita la visualización y comprensión de los datos, lo que permite a los analistas y científicos de datos identificar patrones y tendencias importantes. El PCA es una técnica lineal que transforma las variables correlacionadas en un número menor de variables no correlacionadas llamadas componentes principales. Por otro lado, t-SNE y UMAP son técnicas no lineales que son particularmente útiles para preservar la estructura local de los datos en espacios de baja dimensión, lo que las hace ideales para la visualización de agrupaciones complejas y estructuras de datos de alta dimensión. Estas técnicas son herramientas poderosas que ayudan a simplificar la complejidad de los datos sin perder información crítica, lo que permite un análisis más eficiente y efectivo.

La reducción de la dimensionalidad es un paso crucial en el análisis de datos, especialmente cuando se trata de grandes conjuntos de datos con muchas variables. Esto facilita la visualización y comprensión de los datos, lo que permite identificar patrones y tendencias importantes. Algunas técnicas comunes de reducción dimensional son PCA (\textit{Principal Component Analisis}), t-SNE (\textit{t-Distributed Stochastic Neighbor Embedding}) y UMAP (\textit{Uniform Manifold Approximation and Projection}). Estas técnicas son herramientas poderosas que ayudan a simplificar la complejidad de los datos sin perder información crítica, lo que permite un análisis más eficiente y efectivo.

El t-SNE es un método no linear y estocástico. Su funcionamiento se puede separar en dos etapas. En la primera, se seleccionan los vecinos de cada punto. Para ello se utiliza una distribución gaussiana alrededor de él, donde los más cercanos tienen un probabilidad mayor de ser seleccionados que los lejanos. Este paso permite al modelo preservar las estructuras locales. Durante la segunda etapa, se asignan posiciones iniciales aleatorias en un espacio de menor dimensión (generalmente 2 o 3 dimensiones). Luego, se define una distribución de probabilidad similar para los puntos en el nuevo espacio y se minimiza la divergencia entre las dos distribuciones. Esta etapa ayuda a mantener la fidelidad de la representación en el espacio reducido. De esta forma, el algoritmo logra una transformación de los datos hacia una dimensión reducida que preserva la similitud entre los vecinos cercanos.

UMAP es una técnica muy similar a t-SNE. Una de las diferencias principales es que, durante la selección de los vecinos, se asume que los datos forman una variedad de menor dimensión que el espacio original. Esto le permite ser más eficiente en términos de tiempo de cómputo y más efectivo en la conservación de relaciones a gran escala. Otra característica de este método es que, para hacer la representación reducida, minimiza la entropía cruzada en lugar de la divergencia entre las distribuciones. Estas diferencias permiten a este algoritmo preservar mejor tanto la estructura local como la global de los datos, además de hacerlo capaz de trabajar con datos que no se ajustan necesariamente a una distribución normal. 

Por otro lado, el PCA es una técnica lineal y determinista que transforma variables correlacionadas en un conjunto reducido de variables no correlacionadas, conocidas como componentes principales. Al igual que en los casos anteriores, su funcionamiento se puede dividir en dos etapas. En la primera etapa, se centran los datos en su media aritmética y se calcula la matriz de covarianza. Esta matriz captura la variabilidad conjunta entre múltiples variables aleatorias y permite comprender las relaciones entre ellas. Luego, durante la segunda etapa, se obtienen los autovalores, ordenados de mayor a menor, y sus correspondientes autovectores. Los autovectores representan las direcciones de máxima varianza en los datos, mientras que los autovalores indican la cantidad de varianza que se encuentra en cada una de estas direcciones. Al proyectar los datos originales sobre los primeros autovectores, se obtiene una representación de los datos en un sistema ortogonal que maximiza la conservación de la varianza, utilizando el menor número posible de componentes.

% TODO
t-SNE y UMAP se han vuelto muy populares últimamente debido a su eficiencia y la gran capacidad de para visualizar datos de alta dimensión en un espacio de 2 o 3 dimensiones \alert{[insert cites here]}. Pero su naturaleza no linear y estocástica hace que sea complejo una interpretación cuantitativa de los resultados. Por otro lado, PCA es una técnica lineal y determinista que junto a su relativa sencillez permite realizar varias interpretaciones de sus resultados \alert{[insert cites here]}.

Sin embargo, la aplicación directa del PCA puede presentar algunas complicaciones. Una de las principales dificultades es la construcción de la matriz de covarianza, ya que el número de elementos que contiene es igual al cuadrado de la dimensión original de los datos. Esto hace que sea imposible almacenarla en la memoria RAM de la mayoría de los equipos de cómputo personales. Por ejemplo, si el número de componentes de los datos es $5 * 10 ^4$, la matriz de covarianza tendría $2.5 * 10 ^ 9$ elementos. Suponiendo que cada elemento ocupe 8 bytes, el tamaño total de la matriz sería aproximadamente 19 GB. Si se utiliza el hecho de que esta es una matriz simétrica, se podría guardar solo los elementos de la triangular superior (o inferior) permitiendo reducir el almacenamiento necesario casi a la mitad. Sin embargo, aún así se requeriría mucho espacio y escala rápidamente con la dimensión de los datos. Esto hace que sea necesario recurrir al almacenamiento en disco, que tiene una velocidad de lectura y escritura menor que la memoria RAM.


%En la práctica, se utilizan técnicas como la descomposición en valores singulares (SVD) para trabajar con matrices de covarianza de manera más eficiente, reduciendo así la necesidad de almacenamiento y acelerando el acceso a la información. Además, existen algoritmos y enfoques de big data diseñados para manejar grandes volúmenes de datos que no caben en la memoria RAM y que optimizan el uso del disco duro para minimizar el impacto en el rendimiento.