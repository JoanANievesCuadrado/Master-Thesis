% !TeX root = Tesis.tex
% !TeX spellcheck = es
% !TeX encoding = UTF-8
\chapter{Materiales y métodos}
\label{cap1}
\section{Reducción de la dimensionalidad}
\label{c11}
\onehalfspacing

%La reducción de la dimensionalidad es un paso crucial en el análisis de datos, especialmente cuando se trata de grandes conjuntos de datos con muchas variables. Al reducir la dimensionalidad, se facilita la visualización y comprensión de los datos, lo que permite a los analistas y científicos de datos identificar patrones y tendencias importantes. El PCA es una técnica lineal que transforma las variables correlacionadas en un número menor de variables no correlacionadas llamadas componentes principales. Por otro lado, t-SNE y UMAP son técnicas no lineales que son particularmente útiles para preservar la estructura local de los datos en espacios de baja dimensión, lo que las hace ideales para la visualización de agrupaciones complejas y estructuras de datos de alta dimensión. Estas técnicas son herramientas poderosas que ayudan a simplificar la complejidad de los datos sin perder información crítica, lo que permite un análisis más eficiente y efectivo.

La reducción de la dimensionalidad es un paso crucial en el análisis de datos, especialmente cuando se trata de grandes conjuntos con muchas variables. Este facilita la visualización y comprensión de los datos, lo que permite una rápida identificación de patrones y tendencias importantes. Algunas técnicas comunes de reducción dimensional son PCA (\textit{Principal Component Analisis}), t-SNE (\textit{t-Distributed Stochastic Neighbor Embedding}) y UMAP (\textit{Uniform Manifold Approximation and Projection}). Estas técnicas son herramientas poderosas que ayudan a simplificar la complejidad de los datos sin perder información crítica, lo que permite un análisis más eficiente y efectivo.

El t-SNE es un método no linear y estocástico. Su funcionamiento se puede separar en dos etapas. En la primera, se seleccionan los vecinos de cada punto. Para ello se utiliza una distribución gaussiana alrededor de él, donde los más cercanos tienen un probabilidad mayor de ser seleccionados que los lejanos. Este paso permite al modelo preservar las estructuras locales. Durante la segunda etapa, se asignan posiciones iniciales aleatorias en un espacio de menor dimensión (generalmente 2 o 3 dimensiones). Luego, se define una distribución de probabilidad similar para los puntos en el nuevo espacio y se minimiza la divergencia entre las dos distribuciones. Esta etapa ayuda a mantener la fidelidad de la representación en el espacio reducido. De esta forma, el algoritmo logra una transformación de los datos hacia una dimensión reducida que preserva la similitud entre los vecinos cercanos \cite{Maaten2008, Jung_2024}.

UMAP es una técnica muy similar a t-SNE. Una de las diferencias principales es que, durante la selección de los vecinos, se asume que los datos forman una variedad de menor dimensión que el espacio original. Esto le permite ser más eficiente en términos de tiempo de cómputo y más efectivo en la conservación de relaciones a gran escala. Otra característica de este método es que, para hacer la representación reducida, minimiza la entropía cruzada en lugar de la divergencia entre las distribuciones. Estas diferencias permiten a este algoritmo preservar mejor tanto la estructura local como la global de los datos, además de hacerlo capaz de trabajar con datos que no se ajustan necesariamente a una distribución normal. \cite{McInnes2018, Becht_2018}

Por otro lado, el PCA es una técnica lineal y determinista que transforma variables correlacionadas en un conjunto reducido de variables no correlacionadas, conocidas como componentes principales. Al igual que en los casos anteriores, su funcionamiento se puede dividir en dos etapas. En la primera etapa, se centran los datos en su media aritmética y se calcula la matriz de covarianza. Esta matriz captura la variabilidad conjunta entre múltiples variables aleatorias y permite comprender las relaciones entre ellas. Luego, durante la segunda etapa, se obtienen los autovalores, ordenados de mayor a menor, y sus correspondientes autovectores. Los autovectores representan las direcciones de máxima varianza en los datos, mientras que los autovalores indican la cantidad de varianza que se encuentra en cada una de estas direcciones. Al proyectar los datos originales sobre los primeros autovectores, se obtiene una representación de los datos en un sistema ortogonal que maximiza la conservación de la varianza, utilizando el menor número posible de componentes \cite{Lever2017}.

t-SNE y UMAP se han vuelto muy populares últimamente debido a su eficiencia y la gran capacidad de para visualizar datos de alta dimensión en un espacio de 2 o 3 dimensiones. Pero su naturaleza no linear y estocástica hace que sea complejo una interpretación cuantitativa de los resultados. Por otro lado, PCA es una técnica lineal y determinista que junto a su relativa sencillez permite realizar varias interpretaciones de sus resultados.

Sin embargo, la aplicación directa del PCA puede presentar algunas complicaciones. Una de las principales dificultades es la construcción de la matriz de covarianza, ya que el número de elementos que contiene es igual al cuadrado de la dimensión original de los datos. Esto hace que sea imposible almacenarla en la memoria RAM de la mayoría de los equipos de cómputo personales. Por ejemplo, si el número de componentes de los datos es $5 \times 10^4$, la matriz de covarianza tendría $2.5 \times 10^9$ elementos. Suponiendo que cada elemento ocupe 8 bytes, el tamaño total de la matriz sería aproximadamente 19 GB. Si se hace uso de que esta es una matriz simétrica, se podría guardar solo los elementos de la triangular superior (o inferior), permitiendo reducir el almacenamiento necesario casi a la mitad. Sin embargo, aún así se requeriría mucho espacio y este escala rápidamente con el aumento de la dimensión de los datos. Por lo tanto, en general, es necesario recurrir al almacenamiento en disco, que tiene una velocidad de lectura y escritura menor que la memoria RAM.

Otro problema grave que enfrenta el algoritmo estándar del PCA es el cálculo de los autovalores y autovectores. La mayoría de las implementaciones de los métodos directos usados para este cálculo no permiten que se apliquen a grandes matrices debido a las limitaciones de la memoria RAM. Una característica del PCA que resulta de gran ayuda en esta parte es que, en general, no es necesario calcular todos los autovalores, solo los más grandes y sus correspondientes autovectores.

Un algoritmo relativamente sencillo de implementar y que permite calcular solo los autovalores más grandes y  sus correspondientes autovectores es el método de Lanczos. En el caso del análisis de datos de expresión genética, donde solo un subconjunto diferente de genes se expresa en cada tejido, la matriz de covarianza podría tener un número elevado de valores nulos. Esto es beneficioso para el método de Lanczos, ya que funciona mejor con matrices dispersas.

El método de Lanczos puede ser de gran utilidad en algunos problemas donde las implementaciones estándar pueden verse limitadas. Sin embargo, en la práctica se utilizan otras técnicas para realizar el PCA de forma indirecta. Una de las alternativas es la descomposición en valores singulares (SVD, por sus siglas en inglés). %Por estas razones, a continuación mostraremos brevemente la implementación básica del método de Lanczos. Luego, en qué consiste el SVD y cómo realizar el PCA de forma indirecta a partir de este.
A continuación se describirá brevemente como funciona la SVD, algunas de sus propiedades y como realizar el PCA a partir de esta.

%\subsection*{Algoritmo de Lanczos}\label{subsec:lanczos}
%
%El método de Lanczos es una técnica numérica utilizada para encontrar los autovalores y autovectores de una matriz grande y dispersa. Es especialmente útil en problemas de álgebra lineal donde la matriz es demasiado grande para ser manejada por métodos directos.
%
%%TODO
%\alert{Poner aquí el algoritmo general del método de Lanczos}
%
%Una de las limitaciones de este algoritmo es su estabilidad numérica. \alert{Buscar citas} %TODO
%
%\alert{concluir esta idea y enlazar con la siguiente sección}

\subsection*{Descomposición en valores singulares}\label{subsec:svd}

La SVD provee una descomposición numéricamente estable de matrices que puede ser usado en una gran variedad de propósitos. Como resultado de aplicar este algoritmo se obtiene una descomposición matricial única que existe para toda matriz de valores complejos $\mathbf{X} \in \mathbb{C}^{n \times m}$:

\begin{equation}
	\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^*,
\end{equation}
donde $\mathbf{U} \in \mathbb{C}^{n \times n}$ y $\mathbf{V} \in \mathbb{C}^{m \times m}$ son matrices unitarias con columnas ortonormales, y $\mathbf{\Sigma} \in \mathbb{R}^{n \times m}$ una matriz con valores reales no negativos en la diagonal y ceros fuera de la diagonal. Aquí $^*$ denota la transpuesta conjugada.

Cuando $n \ge m$, la matriz $\mathbf{\Sigma}$ tiene como máximo $m$ valores distintos de cero en la diagonal y puede ser escrita como $\mathbf{\Sigma} = \begin{bmatrix}\hat{\mathbf{\Sigma}} \\ 0\end{bmatrix}$. Por lo tanto, es posible representar $\mathbf{X}$ de forma exacta usando la versión reducida de SVD:

\begin{equation}
	\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^* = \begin{bmatrix} 	\hat{\mathbf{U}} & \mathbf{U}^\perp \end{bmatrix} \begin{bmatrix} \hat{\mathbf{\Sigma}} \\ 0 \end{bmatrix} \mathbf{V}^*,
\end{equation}
las columnas de $\mathbf{U}^\perp$ abarcan un espacio vectorial que es complementario y ortogonal a $\hat{\mathbf{U}}$. Las columnas de $\mathbf{U}$ son llamadas vectores singulares izquierdos de $\mathbf{X}$ y forman una base del espacio de los vectores columnas de $\mathbf{X}$. Las columnas de $\mathbf{V}$ son los vectores singulares derechos y forman una base para los vectores filas de $\mathbf{X}$. Los elementos diagonales de $\hat{\mathbf{\Sigma}} \in \mathbb{C}^{m \times m}$ , los llamados valores singulares, están ordenados de mayor a menor. El rango de $\mathbf{X}$ es igual a la cantidad de valores singulares distintos de cero.

Para ver la relación de esta técnica con el PCA, partimos de la matriz de covarianza. Esta se construye a partir de la siguiente expresión:

\begin{equation}
	\sigma_{ij} = \frac{1}{N - 1}\displaystyle{\sum_{l=1}^{N} \left( x_i^{(l)} - \mu_i \right) \left( x_j^{(l)} - \mu_j \right)},
	\label{eq:covmat1}
\end{equation}

donde $i$ y $j$ son la característica $i$-ésima y $j$-ésima del conjunto de datos estudiado, en nuestro caso son el gen $i$ y $j$, respectivamente. La variable $l$ se recorre por todas las muestras. $\sigma_{ij}$ es el elemento $(i,\ j)$ de la matriz de covarianza, es decir, es la covarianza entre el gen $i$ y el $j$ para los elementos no diagonales y la varianza del gen $i$ para los elementos de la diagonal principal. El número total de muestras es $N$, $x_i^{(l)} \in \mathbb{R}$ es el valor de la componente $i$ en la muestra $l$, y, por último, $\mu_i$ es la media de los valores de la componente $i$ sobre todas las muestras.

Si los datos ya han sido previamente centrados, es decir, se cumple que $\mu_i = 0$ para todo $i$, entonces la ecuación \eqref{eq:covmat1} puede reducirse a:

\begin{equation}
	\sigma_{ij} = \frac{1}{N - 1} \displaystyle{ \sum_{l=1}^{N} x_i^{(l)} x_j^{(l)} },
	\label{eq:covmat2}
\end{equation}

Si definimos una matriz $\mathbf{X} \in \mathbb{R}^{n \times m}$, tal que el elemento $(i,\ j)$ es igual a $x_i^{(j)}/(N-1)$, la ecuación \eqref{eq:covmat2} queda representada en forma matricial como:

\begin{equation}
	\mathbf{C} = \mathbf{X}^T  \mathbf{X}.
\end{equation}

Si usamos la SVD de $\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T$ y la propiedad de ortonormalidad de $\mathbf{U}$ y $\mathbf{V}$, obtenemos:

\begin{eqnarray}
	\mathbf{C} =& \mathbf{V} \mathbf{\Sigma} \mathbf{U}^T \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T, \\
	=& \mathbf{V} \mathbf{\Sigma}^2 \mathbf{V}^T,
\end{eqnarray}
es decir, $\mathbf{\Sigma}$ y $\mathbf{V}$ son la solución del siguiente problema de autovalores:
\begin{equation}
	\mathbf{C} \mathbf{V} = \mathbf{V} \mathbf{\Sigma}^2.
\end{equation}

En otras palabras, cada valor singular de $\mathbf{X}$ distinto de cero es la raíz cuadrada positiva de los autovalores de su matriz de covarianza, y las columnas de $\mathbf{V}$ son los autovectores. En términos del PCA, las columnas de $\mathbf{V}$ son las componentes principales de $\mathbf{X}$ y los elementos de la diagonal de $\mathbf{\Sigma}^2$ representan la varianza de los datos en cada una de las componente.

Esta es la manera usual en la que los algoritmo actuales realizan el PCA, por ejemplo, la librería \emph{scikit-learn} de Python \cite{scikit-learn}. En general, nosotros preferimos usar directamente SVD para hacer el PCA, ya que brinda mayor flexibilidad y control. Por ejemplo, si en lugar usar una matriz de media cero, los datos se centran en el valor medio de un subconjunto, las componentes principales van a indicar la dirección en la cual hay más dispersión respecto a este subjconjunto.

Una ventaja significativa de este método para realizar el PCA, es que se obtiene las componentes principales directamente, sin tener que calcular la matriz de covarianza. Esto permite ahorrar tiempo y recursos computacionales, algo que toma mayor importancia en la medida que aumenta la cantidad de datos y el números de variables a procesar. Un procedimiento similar se siguió en las referencias \cite{Gonzalez_2023, Gonzalez_2021}.

\section{Datos de expresión genética}
Utilizamos datos de expresión genética obtenidos de dos experimentos distintos. El primero de ellos contiene muestras patológicamente normales (o ``sanas'') y con glioma, y fueron tomadas del Atlas del Genoma del Cáncer (TCGA, \href{https://www.cancer.gov/tcga}{https://www.cancer.gov/tcga}) \cite{Brennan_2013, Tomczak2015}. Estas son tomadas durante procedimientos quirúrgicos. Los tumores se pueden localizar en diferente zonas cerebrales pero, como es común en el glioma, estos son tumores tomados de la sustancia blanca del cerebro \cite{ellingson2013probabilistic}.

Hay 5 muestras normales de pacientes con edades en el rango entre 49 y 74 años, mientras que el intervalo de edades de las 169 muestras de glioma es entre 21 y 89 años. Las pacientes femeninas representan aproximadamente dos tercios de la cohorte.

El segundo grupo de datos proviene del estudio longitudinal del Instituto Allen sobre el envejecimiento y la demencia (\href{https://aging.brain-map.org/}{https://aging.brain-map.org/}) \cite{Miller_2017}. Las muestras son tomadas \textit{post mortem}. El grupo de control está conformado por 47 muestras, mientras que en el otro hay 28 muestras. El intervalo de edad para todas estas muestras es entre 77 y 101 años. El diagnóstico de la enfermedad del Alzheimer está respaldado por pruebas cognitivas y otras pruebas clínicas. Alrededor del 40 \% de la cohorte son mujeres.

En ambos experimentos los valores de expresión genética se encuentran en unidades FPKM (\textit{fragments per kilobase of transcript per million fragments mapped}), que es una unidad común en este tipo de estudios. En términos simples, dicha unidad de medida significa: la tasa de fragmentos por base multiplicada por un número muy grande ($ 10^9 $). El cálculo de FPKM para el gen $ i $ se realiza por medio de la siguiente fórmula \cite{Zhao_2021}:

\begin{equation}
	\begin{split}
		FPKM_i &= \frac{q_i}{(l_i/10^3) (\sum_{j}q_j/10^6)} ,\\
		&= \frac{q_i}{l_i \sum_j q_j} * 10^9,
	\end{split}
\end{equation}
donde $ q_i $ es la cantidad de fragmentos contados, $ l_i $ es la longitud del gen, y $ \sum_j q_j $ corresponde al número total de fragmentos.

El uso de esta unidad de medida puede resultar complicado debido a que el valor de expresión genética de muchos genes es cero o muy cercano a cero, mientras que un conjunto muy reducido puede presentar valores entre $10^2$ y $10^4$. Por esta razón, en muchos estudios de análisis de expresión genética se utiliza una variable conocida como expresión diferencial logarítmica ($e_{fold}$). Para un gen determinado $i$, esta se calcula de la siguiente manera:
\begin{equation}
	e^i_{fold} = \log_2\left(\frac{e^i}{e^{i}_{ref}}\right),
\end{equation}
donde $e^i$ es la expresión del gen $i$ y $e^i_{ref}$ es un valor de referencia para dicho gen, ambos en unidades FPKM. Esta nueva variable permite concentrar mejor los puntos, y su distribución tiende a asemejarse a una distribución gaussiana. 

Usualmente, empleamos como valor de referencia del gen $i$ la media geométrica de su expresión en el conjunto de muestras normales (o ``sanas''), tras haber sido desplazadas por una constante pequeña, generalmente $0.1$ o $0.01$. De esta forma, los valores negativos o positivos de la expresión diferencial logarítmica indican la subexpresión o sobreexpresión de un gen, respectivamente.