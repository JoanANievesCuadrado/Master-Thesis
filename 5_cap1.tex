% !TeX root = Tesis.tex
% !TeX spellcheck = es_ES
% !TeX encoding = UTF-8
\chapter{Materiales y métodos}
\label{cap1}
\section{Reducción de la dimensionalidad}
\label{c11}
\onehalfspacing

%La reducción de la dimensionalidad es un paso crucial en el análisis de datos, especialmente cuando se trata de grandes conjuntos de datos con muchas variables. Al reducir la dimensionalidad, se facilita la visualización y comprensión de los datos, lo que permite a los analistas y científicos de datos identificar patrones y tendencias importantes. El PCA es una técnica lineal que transforma las variables correlacionadas en un número menor de variables no correlacionadas llamadas componentes principales. Por otro lado, t-SNE y UMAP son técnicas no lineales que son particularmente útiles para preservar la estructura local de los datos en espacios de baja dimensión, lo que las hace ideales para la visualización de agrupaciones complejas y estructuras de datos de alta dimensión. Estas técnicas son herramientas poderosas que ayudan a simplificar la complejidad de los datos sin perder información crítica, lo que permite un análisis más eficiente y efectivo.

La reducción de la dimensionalidad es un paso crucial en el análisis de datos, especialmente cuando se trata de grandes conjuntos de datos con muchas variables. Al reducir la dimensionalidad, se facilita la visualización y comprensión de los datos, lo que permite identificar patrones y tendencias importantes. Algunas técnicas comunes de reducción dimensional son PCA (\textit{Principal Component Analisis}), t-SNE (\textit{t-Distributed Stochastic Neighbor Embedding}) y UMAP (\textit{Uniform Manifold Approximation and Projection}). Estas técnicas son herramientas poderosas que ayudan a simplificar la complejidad de los datos sin perder información crítica, lo que permite un análisis más eficiente y efectivo.

El t-SNE es un método no linear y estocástico. Su funcionamiento se puede separar en dos etapas. En la primera, se seleccionan los vecinos de cada punto. Para ello se utiliza una distribución gaussiana alrededor de él, donde los más cercanos tienen un probabilidad mayor de ser seleccionados que los lejanos. Este paso permite al modelo preservar las estructuras locales. Durante la segunda etapa, se asignan posiciones iniciales aleatorias en un espacio de menor dimensión (generalmente 2 o 3 dimensiones). Luego, se define una distribución de probabilidad similar para los puntos en el nuevo espacio y se minimiza la divergencia entre las dos distribuciones. Esta etapa ayuda a mantener la fidelidad de la representación en el espacio reducido. De esta forma, el algoritmo logra una transformación de los datos hacia una dimensión reducida que preserva la similitud entre los vecinos cercanos.

UMAP es una técnica muy similar a t-SNE. Una de las diferencias principales es que, durante la selección de los vecinos, se asume que los datos forman una variedad de menor dimensión que el espacio original. Esto le permite ser más eficiente en términos de tiempo de cómputo y más efectivo en la conservación de relaciones a gran escala. Otra característica de este método es que, para hacer la representación reducida, minimiza la entropía cruzada en lugar de la divergencia entre las distribuciones. Estas diferencias permiten a este algoritmo preservar mejor tanto la estructura local como la global de los datos, además de hacerlo capaz de trabajar con datos que no se ajustan necesariamente a una distribución normal. 

Por otro lado, el PCA es una técnica lineal y determinista que transforma variables correlacionadas en un conjunto reducido de variables no correlacionadas, conocidas como componentes principales. Al igual que en los casos anteriores, su funcionamiento se puede dividir en dos etapas. En la primera etapa, se centran los datos en su media aritmética y se calcula la matriz de covarianza. Esta matriz captura la variabilidad conjunta entre múltiples variables aleatorias y permite comprender las relaciones entre ellas. Luego, durante la segunda etapa, se obtienen los autovalores, ordenados de mayor a menor, y sus correspondientes autovectores. Los autovectores representan las direcciones de máxima varianza en los datos, mientras que los autovalores indican la cantidad de varianza que se encuentra en cada una de estas direcciones. Al proyectar los datos originales sobre los primeros autovectores, se obtiene una representación de los datos en un sistema ortogonal que maximiza la conservación de la varianza, utilizando el menor número posible de componentes.


Sin embargo, su tamaño suele ser muy grande y el espacio que ocupa su almacenamiento es superior a la capacidad de la memoria RAM de los ordenadores comunes, lo que hace necesario guardarla en el disco duro, provocando que el acceso a esta información sea relativamente lento 


%En la práctica, se utilizan técnicas como la descomposición en valores singulares (SVD) para trabajar con matrices de covarianza de manera más eficiente, reduciendo así la necesidad de almacenamiento y acelerando el acceso a la información. Además, existen algoritmos y enfoques de big data diseñados para manejar grandes volúmenes de datos que no caben en la memoria RAM y que optimizan el uso del disco duro para minimizar el impacto en el rendimiento.